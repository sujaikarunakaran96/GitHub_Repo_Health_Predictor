{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cloudbakers_Data_Modelling.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WiZsVDM_HE1S"},"source":["#Cloudbakers Data Modelling Script"]},{"cell_type":"code","metadata":{"id":"9zYA7qcWBpFu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620247691398,"user_tz":300,"elapsed":285,"user":{"displayName":"Chaitra Srirama","photoUrl":"","userId":"06885739003473900788"}},"outputId":"1737e24f-99f1-4359-fabf-6daae25c18d8"},"source":["### initalizing libraries \n","import numpy as np, pandas as pd, matplotlib.pyplot as plt, keras, itertools\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout\n","\n","### Authorize to mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/My Drive/Capstone Project./Datasets')\n","\n","### Read the dataset\n","Dataset = pd.read_csv('Dataset_RF_Activity_Score.csv')\n","repo_data=Dataset.loc[Dataset['repoID']==113564765]\n","single_repo_series=repo_data.Normalised_Activity_Score\n","Risk_Score=single_repo_series\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X00vRircLHgd"},"source":["### Function for Grid Search and LSTM Model Building\n","\n","def GridSearch(series, params_grid):\n","    \"\"\"\n","    Runs a grid search over specified parameter ranges\n","    Args:\n","        series: the time series of interest\n","        params_grid: a dictionary specifying parameters\n","                    {input_size, hidden_units, dropout,\n","                    learning_rate, n_ahead, val_split,\n","                    epochs, verbose, plot} and their\n","                    possible value ranges\n","    Returns:\n","        model: the model with the lowest MSE\n","        logs: logs of all combinations\n","\n","    \"\"\"\n","    param_names = list(params_grid.keys())\n","    param_values = list(params_grid.values()) \n","    combinations = list(itertools.product(*param_values))\n","    \n","    logs = pd.DataFrame(combinations,columns=param_names)\n","    \n","    mse_prev = 50\n","    for index, comb in enumerate(combinations):\n","        print('Fitting {}/{} model'.format(index+1,len(combinations)))\n","        params = dict(zip(param_names,comb))\n","        model, mse, history, predn, y_test = FitEvaluate(series,params) #Fit eval\n","        train_loss = history.history['loss']\n","        val_loss = history.history['val_loss']\n","                \n","        if mse < mse_prev:\n","            mse_prev = mse\n","            best_model = model              #best Model\n","            best_predictions = predn\n","            yt = y_test\n","        \n","        logs.at[index,'mse'] = mse\n","        logs.at[index,'mean_training_loss'] = np.mean(train_loss)\n","        logs.at[index,'std_training_loss'] = np.std(train_loss)\n","        logs.at[index,'mean_val_loss'] = np.mean(val_loss)\n","        logs.at[index,'std_val_loss'] = np.std(val_loss)\n","    logs.to_csv('results.csv',index=False)\n","    # view_predictions(series,best_predictions,y_test,'Actual vs Forecast')\n","    return best_model, logs, best_predictions, yt\n","\n","def FitEvaluate(time_series, params):\n","    \"\"\"\n","    Calls the pipeline to fit an LSTM model to the\n","    given time series\n","\n","    Args:\n","        time_series: the time series of interest\n","        params: a dictionary specifying parameters\n","                {input_size, hidden_units, dropout,\n","                learning_rate, n_ahead, val_split,\n","                epochs, verbose, plot}\n","    Returns:\n","        model: keras sequential model\n","        mse: mean squared error of the prediction\n","        history: training and validation loss history\n","\n","    \"\"\"\n","    for k in params.keys():\n","        globals()[k] = params[k]\n","        \n","    scaled_series, scaler = preprocessing(time_series) \n","    series, y_test, n_test = getSeries(scaled_series,0.8)\n","    X_train,y_train,X_test = getInputOutput(series,input_size)\n","    \n","    # show only n_ahead number of actual values\n","    y_test = y_test[np.arange(n_ahead)]\n","\n","    new_model, predictions, history = FitForecast(X_train,y_train,X_test,n_ahead,\n","                                        input_size,hidden_units,dropout, val_split,\n","                                        learning_rate,epochs,trained_model=None)\n","    \n","    # rescaling\n","    series = inverse_transform(series, scaler)\n","    y_test = inverse_transform(y_test, scaler)\n","    predictions = inverse_transform(predictions, scaler)\n","\n","    mse = mean_squared_error(y_true=y_test,y_pred=predictions)\n","    \n","    #if verbose:\n","     # print('\\n')\n","      #print('======== Prediction Evaluation =========')\n","      #print('MSE is {}'.format(mse))\n","        \n","    #if plot:\n","     # ViewLoss(history)\n","      #view_predictions(series,predictions,y_test,'Actual vs Forecast')\n","    return new_model, mse, history, predictions, y_test\n","\n","\n","def preprocessing(series):\n","    \"\"\"\n","    MinMax Scaling of the raw time series\n","    Args:\n","        series: the raw time series\n","    Returns:\n","        scaled_series and scaler object\n","    \"\"\"\n","    series = np.array(series)\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    scaled = scaler.fit_transform(series.reshape(-1, 1))\n","    scaled_series = scaled.reshape((len(series),))\n","    return (\n","     scaled_series, scaler)\n","\n","\n","def getSeries(data, p):\n","    \"\"\"\n","    Splits a given time series proportionally\n","    for training and testing purposes\n","\n","    Args:\n","        data: numpy array or pandas series\n","              containing the time series.\n","        p: float value that defines the\n","           proportion of the series used\n","           for training.\n","    Returns:\n","        series: time series for training\n","        y_test: time series for testing\n","        n_test: number of timesteps\n","                in the test series\n","\n","    \"\"\"\n","    n = data.shape[0]\n","    n_train = int(n * p)\n","    n_test = n - n_train\n","    x = np.arange(n)\n","    index_train = x[:n_train]\n","    index_test = x[n_train:]\n","    series = data[index_train]\n","    y_test = data[index_test]\n","    return (series, y_test, n_test)\n","\n","\n","\n","def getInputOutput(series, input_size):\n","    \"\"\"\n","    Transforms the time series into desired\n","    shape to be able to pass to the network\n","\n","    Args:\n","        series: the time series.\n","        input_size: int that defines the length\n","                    of the input sequence to be\n","                    fed to the network\n","    Returns:\n","        X_train: input dataset\n","        y_train: output values\n","        X_test: the last available sequence\n","\n","    \"\"\"\n","    series = np.array(series)\n","    xlen = len(series)\n","    xrows = xlen - input_size\n","    X_train, y_train = [], []\n","    for i in range(xrows):\n","        j = i + input_size\n","        a = series[i:j, np.newaxis]\n","        X_train.append(a)\n","        y_train.append(series[j])\n","\n","    X_train, y_train = np.array(X_train), np.array(y_train)\n","    X_test = series[xrows:].reshape(1, input_size, 1)\n","    return (\n","     X_train, y_train, X_test)\n","    \n","\n","\n","def FitForecast(X_train, y_train, X_test, n_ahead, input_size, hidden_units, dropout, val_split, learning_rate, epochs, trained_model):\n","    \"\"\"\n","    Fits a model and returns the predicted values.\n","    Optionally weights from another network can be passed\n","\n","    Args:\n","        X_train: input dataset for training\n","        y_train: output dataset for training\n","        X_test: the last available sequence\n","        n_ahead: number of predictions to make\n","        time_series: the time series of interest\n","        input_size: int that defines the length\n","                    of the input sequence to be\n","                    fed to the network\n","        hidden_units: int/list specifying the number\n","                      of hidden units in the hidden\n","                      layer/layers\n","        dropout: boolean specifing whether to add dropout\n","                 with 0.5 rate per layer\n","        learning_rate: learning rate of the Adam\n","                       optimization algorithm\n","        epochs: int that defines the number of\n","                training phases through the\n","                training dataset\n","        trained_model: already trained keras sequential\n","                       model\n","\n","    Returns:\n","        model: keras sequential model\n","        predictions: numpy array containing the predicted values\n","        history: training and validation loss history\n","\n","    \"\"\"\n","    model = build_LSTM(input_size,hidden_units,dropout, learning_rate)\n","    \n","    if trained_model is not None:\n","        model.set_weights(weights = trained_model.get_weights())        \n","    \n","    \n","    history = model.fit(x=X_train, y=y_train, \n","                batch_size=1, epochs=epochs, \n","                verbose=1, validation_split=val_split,\n","                shuffle=False)\n","\n","    predictions = predict_ahead(model,X_test,n_ahead)\n","    return model, predictions, history\n","\n","\n","\n","def build_LSTM(input_size, hidden_units, dropout, learning_rate):\n","    \"\"\"\n","    Builds the Network with LSTM hidden layers\n","\n","    Args:\n","        input_size: int that defines the length\n","                    of the input sequence to be\n","                    fed to the network\n","        hidden_units: int/list specifying the number\n","                      of hidden units in the hidden\n","                      layer/layers\n","        dropout: boolean specifing whether to add dropout\n","                 with 0.5 rate per layer\n","        learning_rate: learning rate of the Adam\n","                       optimization algorithm\n","    Returns:\n","        model: keras sequential model\n","\n","    \"\"\"\n","    h = hidden_units\n","    \n","\n","    model = Sequential()\n","    \n","    if isinstance(h,list):\n","    \n","        model.add(LSTM(h[0],\n","                   batch_input_shape=(1,input_size, 1), \n","                   return_sequences=True, \n","                   stateful=True))\n","                  \n","        if dropout:\n","            model.add(Dropout(rate=0.5))\n","\n","        if len(h) > 2:\n","            #removing 1st and last units\n","            for index, units in enumerate(h[1:-1]):  \n","                model.add(LSTM(units, \n","                               batch_input_shape=(1,h[index], 1), \n","                               return_sequences=True, \n","                               stateful=True)) \n","                if dropout:\n","                    model.add(Dropout(rate=0.5))\n","\n","        model.add(LSTM(h[-1], \n","                       batch_input_shape=(1,h[-2], 1), \n","                       return_sequences=False, \n","                       stateful=True))\n","        if dropout:\n","            model.add(Dropout(rate=0.5))\n","    else:\n","        model.add(LSTM(h, \n","                   batch_input_shape=(1,input_size, 1),\n","                   return_sequences=False, \n","                   stateful=True)) \n","        if dropout:\n","            model.add(Dropout(rate=0.5))\n","        \n","    \n","    model.add(Dense(1))\n","    adam = keras.optimizers.Adam(lr=learning_rate)\n","    model.compile(loss='mse', optimizer=adam)\n","    return model\n","\n","\n","\n","def predict_ahead(model, X_test, n_ahead):\n","    \"\"\"\n","    Makes predictions based on the last available sequence\n","\n","    Args:\n","        model: keras sequential model\n","        X_test: the last available sequence\n","        n_ahead: number of predictions to make\n","\n","    Returns:\n","        predictions: numpy array containing the predicted values\n","\n","    \"\"\"\n","    predictions = np.zeros(n_ahead)\n","    predictions[0] = model.predict(X_test,batch_size = 1)\n","    \n","    if n_ahead > 1:\n","        for i in range(1,n_ahead):\n","            x_new = np.append(X_test[0][1:],predictions[i-1])\n","            X_test = x_new.reshape(1,x_new.shape[0],1)\n","            predictions[i] = model.predict(X_test,batch_size = 1)\n","    return predictions\n","\n","\n","\n","def inverse_transform(series, scaler):\n","    \"\"\"\n","    Inverse transform of scales series\n","    Args:\n","        series: scaled series\n","        scaler: scaler object\n","    Returns:\n","        unscaled series\n","    \"\"\"\n","    return scaler.inverse_transform(series.reshape(-1, 1))\n","\n","\n","\n","def ViewLoss(history):\n","    '''\n","    Plots the history of model training\n","    '''\n","    plt.plot(history.history['loss'],label='Train')\n","    plt.plot(history.history['val_loss'],label='Val')\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend()\n","    plt.savefig('loss-history.png')\n","    plt.show()\n","\n","\n","def view_predictions(series,predictions,actual,title):\n","    '''\n","    Plots the results of the predictions made by the model\n","    \n","    Args:\n","        series: the time series used for training the network\n","        predictions: numpy array containing the predicted values\n","        actual: the actual time series not seen by the network\n","        title: the title of the plot\n","        \n","    Returns:\n","        plot        \n","        \n","    '''  \n","    \n","    plt.figure(figsize=(8,4))\n","    plt.title(title)\n","    \n","    if isinstance(series,list):\n","        train_index = np.arange(len(series[0]))\n","        test_index = len(series[0]) + np.arange(len(actual))\n","        \n","        plt.plot(train_index,series[0], label = 'general')\n","        \n","    else:\n","        train_index = np.arange(len(series))\n","        test_index = len(series) + np.arange(len(actual))        \n","        plt.plot(train_index,series,label = 'training')\n","\n","    if len(predictions) > 4:\n","        plt.plot(test_index,predictions,label = 'prediction',color='g')\n","        plt.plot(test_index,actual,label = 'actual',color='orange')\n","    else:\n","        plt.scatter(test_index,predictions,label = 'prediction',color='g')\n","        plt.scatter(test_index,actual,label = 'actual',color='orange')    \n","    \n","    plt.xlabel('Index')\n","    plt.ylabel('Data')\n","    \n","    plt.legend(loc='upper left')\n","    plt.savefig('{}_{}.png'.format(title,len(series)))\n","    plt.show()\n","\n","\n","\n","def Test_FitEvaluate(time_series,params,model_):\n","    '''\n","    Calls the pipeline to fit an LSTM model to the \n","    given time series\n","    \n","    Args:\n","        time_series: the time series of interest\n","        params: a dictionary specifying parameters\n","                {input_size, hidden_units, dropout,\n","                learning_rate, n_ahead, val_split, \n","                epochs, verbose, plot}\n","    Returns:\n","        model: keras sequential model      \n","        mse: mean squared error of the prediction\n","        history: training and validation loss history\n","        \n","    '''   \n","    \n","    for k in params.keys():\n","        globals()[k] = params[k]\n","    \n","    \n","    scaled_series, scaler = preprocessing(time_series)\n","    series, y_test, n_test = getSeries(scaled_series,0.8)\n","    X_train,y_train,X_test = getInputOutput(series,input_size)\n","    \n","    # show only n_ahead number of actual values\n","    y_test = y_test[np.arange(n_ahead)]\n","\n","    new_model, predictions, history = FitForecast(X_train,y_train,X_test,n_ahead,\n","                                        input_size,hidden_units,dropout, val_split,\n","                                        learning_rate,epochs,trained_model=model_)\n","    # rescaling\n","    series = inverse_transform(series, scaler)\n","    y_test = inverse_transform(y_test, scaler)\n","    predictions = inverse_transform(predictions, scaler)\n","    \n","    mse = mean_squared_error(y_true=y_test,y_pred=predictions)\n","    \n","    #print('MSE is {}'.format(mse))\n","     \n","    #ViewLoss(history)\n","    #view_predictions(series,predictions,y_test,'Actual vs Forecast')\n","    return new_model, mse, history, predictions, y_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KSwk6S2L-91","executionInfo":{"status":"ok","timestamp":1620247978446,"user_tz":300,"elapsed":110159,"user":{"displayName":"Chaitra Srirama","photoUrl":"","userId":"06885739003473900788"}},"outputId":"f5e932f6-fdaa-494a-db11-5eff10e03fcc"},"source":["### Initialize parameters for Grid Search\n","params_grid = {'input_size': [15, 20, 25, 50],\n","              'hidden_units':[100, [100,50,50], [100,50]],\n","              'dropout': [True],\n","              'learning_rate':[4e-5],\n","              'n_ahead':[12],\n","              'val_split': [0.2],\n","              'epochs':[20],\n","              'verbose':[False],\n","              'plot':[False]}\n","\n","\n","#### Training the model using Grid Search\n","model_, logs, best_predictions, y_test = GridSearch(Risk_Score,params_grid)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 1/12 model\n","Epoch 1/20\n","54/54 [==============================] - 2s 17ms/step - loss: 0.1105 - val_loss: 0.0400\n","Epoch 2/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0811 - val_loss: 0.0338\n","Epoch 3/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0671 - val_loss: 0.0301\n","Epoch 4/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0516 - val_loss: 0.0289\n","Epoch 5/20\n","54/54 [==============================] - 0s 7ms/step - loss: 0.0433 - val_loss: 0.0297\n","Epoch 6/20\n","54/54 [==============================] - 0s 7ms/step - loss: 0.0416 - val_loss: 0.0314\n","Epoch 7/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0459 - val_loss: 0.0325\n","Epoch 8/20\n","54/54 [==============================] - 0s 7ms/step - loss: 0.0369 - val_loss: 0.0340\n","Epoch 9/20\n","54/54 [==============================] - 0s 7ms/step - loss: 0.0422 - val_loss: 0.0338\n","Epoch 10/20\n","54/54 [==============================] - 0s 7ms/step - loss: 0.0415 - val_loss: 0.0339\n","Epoch 11/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0443 - val_loss: 0.0341\n","Epoch 12/20\n","54/54 [==============================] - 0s 7ms/step - loss: 0.0496 - val_loss: 0.0338\n","Epoch 13/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0432 - val_loss: 0.0341\n","Epoch 14/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0512 - val_loss: 0.0332\n","Epoch 15/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0473 - val_loss: 0.0337\n","Epoch 16/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0343 - val_loss: 0.0341\n","Epoch 17/20\n","54/54 [==============================] - 0s 7ms/step - loss: 0.0392 - val_loss: 0.0344\n","Epoch 18/20\n","54/54 [==============================] - 0s 7ms/step - loss: 0.0425 - val_loss: 0.0344\n","Epoch 19/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0463 - val_loss: 0.0339\n","Epoch 20/20\n","54/54 [==============================] - 0s 8ms/step - loss: 0.0478 - val_loss: 0.0343\n","Fitting 2/12 model\n","Epoch 1/20\n","54/54 [==============================] - 6s 37ms/step - loss: 0.0996 - val_loss: 0.0364\n","Epoch 2/20\n","54/54 [==============================] - 1s 17ms/step - loss: 0.0699 - val_loss: 0.0309\n","Epoch 3/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0515 - val_loss: 0.0307\n","Epoch 4/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0469 - val_loss: 0.0332\n","Epoch 5/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0457 - val_loss: 0.0346\n","Epoch 6/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0445 - val_loss: 0.0344\n","Epoch 7/20\n","54/54 [==============================] - 1s 17ms/step - loss: 0.0431 - val_loss: 0.0341\n","Epoch 8/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0472 - val_loss: 0.0340\n","Epoch 9/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0435 - val_loss: 0.0345\n","Epoch 10/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0498 - val_loss: 0.0339\n","Epoch 11/20\n","54/54 [==============================] - 1s 17ms/step - loss: 0.0460 - val_loss: 0.0348\n","Epoch 12/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0588 - val_loss: 0.0341\n","Epoch 13/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0398 - val_loss: 0.0359\n","Epoch 14/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0417 - val_loss: 0.0356\n","Epoch 15/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0440 - val_loss: 0.0351\n","Epoch 16/20\n","54/54 [==============================] - 1s 17ms/step - loss: 0.0391 - val_loss: 0.0346\n","Epoch 17/20\n","54/54 [==============================] - 1s 16ms/step - loss: 0.0458 - val_loss: 0.0347\n","Epoch 18/20\n","54/54 [==============================] - 1s 15ms/step - loss: 0.0461 - val_loss: 0.0360\n","Epoch 19/20\n","54/54 [==============================] - 1s 17ms/step - loss: 0.0409 - val_loss: 0.0363\n","Epoch 20/20\n","54/54 [==============================] - 1s 15ms/step - loss: 0.0457 - val_loss: 0.0365\n","Fitting 3/12 model\n","Epoch 1/20\n","54/54 [==============================] - 4s 27ms/step - loss: 0.0941 - val_loss: 0.0346\n","Epoch 2/20\n","54/54 [==============================] - 1s 11ms/step - loss: 0.0647 - val_loss: 0.0304\n","Epoch 3/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0470 - val_loss: 0.0313\n","Epoch 4/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0488 - val_loss: 0.0326\n","Epoch 5/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0499 - val_loss: 0.0337\n","Epoch 6/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0569 - val_loss: 0.0338\n","Epoch 7/20\n","54/54 [==============================] - 1s 13ms/step - loss: 0.0424 - val_loss: 0.0346\n","Epoch 8/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0466 - val_loss: 0.0344\n","Epoch 9/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0476 - val_loss: 0.0346\n","Epoch 10/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0553 - val_loss: 0.0338\n","Epoch 11/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0463 - val_loss: 0.0347\n","Epoch 12/20\n","54/54 [==============================] - 1s 13ms/step - loss: 0.0542 - val_loss: 0.0336\n","Epoch 13/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0419 - val_loss: 0.0342\n","Epoch 14/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0407 - val_loss: 0.0347\n","Epoch 15/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0461 - val_loss: 0.0342\n","Epoch 16/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0455 - val_loss: 0.0343\n","Epoch 17/20\n","54/54 [==============================] - 1s 11ms/step - loss: 0.0432 - val_loss: 0.0350\n","Epoch 18/20\n","54/54 [==============================] - 1s 13ms/step - loss: 0.0478 - val_loss: 0.0347\n","Epoch 19/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0417 - val_loss: 0.0353\n","Epoch 20/20\n","54/54 [==============================] - 1s 12ms/step - loss: 0.0354 - val_loss: 0.0354\n","Fitting 4/12 model\n","Epoch 1/20\n","50/50 [==============================] - 3s 21ms/step - loss: 0.0505 - val_loss: 0.0378\n","Epoch 2/20\n","50/50 [==============================] - 0s 10ms/step - loss: 0.0412 - val_loss: 0.0324\n","Epoch 3/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0328 - val_loss: 0.0296\n","Epoch 4/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0303 - val_loss: 0.0290\n","Epoch 5/20\n","50/50 [==============================] - 0s 10ms/step - loss: 0.0297 - val_loss: 0.0296\n","Epoch 6/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0274 - val_loss: 0.0304\n","Epoch 7/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0348 - val_loss: 0.0306\n","Epoch 8/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0291 - val_loss: 0.0316\n","Epoch 9/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0346 - val_loss: 0.0316\n","Epoch 10/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0280 - val_loss: 0.0317\n","Epoch 11/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0315 - val_loss: 0.0316\n","Epoch 12/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0372 - val_loss: 0.0312\n","Epoch 13/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0274 - val_loss: 0.0311\n","Epoch 14/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0297 - val_loss: 0.0320\n","Epoch 15/20\n","50/50 [==============================] - 0s 8ms/step - loss: 0.0322 - val_loss: 0.0322\n","Epoch 16/20\n","50/50 [==============================] - 0s 8ms/step - loss: 0.0350 - val_loss: 0.0319\n","Epoch 17/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0336 - val_loss: 0.0317\n","Epoch 18/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0250 - val_loss: 0.0317\n","Epoch 19/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0309 - val_loss: 0.0317\n","Epoch 20/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0291 - val_loss: 0.0321\n","Fitting 5/12 model\n","Epoch 1/20\n","50/50 [==============================] - 6s 42ms/step - loss: 0.0567 - val_loss: 0.0404\n","Epoch 2/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0400 - val_loss: 0.0345\n","Epoch 3/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0296 - val_loss: 0.0331\n","Epoch 4/20\n","50/50 [==============================] - 1s 20ms/step - loss: 0.0319 - val_loss: 0.0335\n","Epoch 5/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0323 - val_loss: 0.0341\n","Epoch 6/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0286 - val_loss: 0.0345\n","Epoch 7/20\n","50/50 [==============================] - 1s 18ms/step - loss: 0.0358 - val_loss: 0.0337\n","Epoch 8/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0316 - val_loss: 0.0345\n","Epoch 9/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0334 - val_loss: 0.0339\n","Epoch 10/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0300 - val_loss: 0.0346\n","Epoch 11/20\n","50/50 [==============================] - 1s 18ms/step - loss: 0.0338 - val_loss: 0.0339\n","Epoch 12/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0269 - val_loss: 0.0351\n","Epoch 13/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0371 - val_loss: 0.0346\n","Epoch 14/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0291 - val_loss: 0.0346\n","Epoch 15/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0323 - val_loss: 0.0347\n","Epoch 16/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0314 - val_loss: 0.0343\n","Epoch 17/20\n","50/50 [==============================] - 1s 18ms/step - loss: 0.0275 - val_loss: 0.0342\n","Epoch 18/20\n","50/50 [==============================] - 1s 18ms/step - loss: 0.0364 - val_loss: 0.0335\n","Epoch 19/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0299 - val_loss: 0.0342\n","Epoch 20/20\n","50/50 [==============================] - 1s 19ms/step - loss: 0.0263 - val_loss: 0.0341\n","Fitting 6/12 model\n","Epoch 1/20\n","50/50 [==============================] - 5s 31ms/step - loss: 0.0501 - val_loss: 0.0353\n","Epoch 2/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0335 - val_loss: 0.0314\n","Epoch 3/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0326 - val_loss: 0.0307\n","Epoch 4/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0300 - val_loss: 0.0316\n","Epoch 5/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0315 - val_loss: 0.0328\n","Epoch 6/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0435 - val_loss: 0.0321\n","Epoch 7/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0353 - val_loss: 0.0327\n","Epoch 8/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0340 - val_loss: 0.0323\n","Epoch 9/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0331 - val_loss: 0.0321\n","Epoch 10/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0302 - val_loss: 0.0320\n","Epoch 11/20\n","50/50 [==============================] - 1s 13ms/step - loss: 0.0290 - val_loss: 0.0324\n","Epoch 12/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0358 - val_loss: 0.0328\n","Epoch 13/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0262 - val_loss: 0.0334\n","Epoch 14/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0334 - val_loss: 0.0333\n","Epoch 15/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0378 - val_loss: 0.0324\n","Epoch 16/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0309 - val_loss: 0.0327\n","Epoch 17/20\n","50/50 [==============================] - 1s 15ms/step - loss: 0.0299 - val_loss: 0.0328\n","Epoch 18/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0297 - val_loss: 0.0325\n","Epoch 19/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0291 - val_loss: 0.0318\n","Epoch 20/20\n","50/50 [==============================] - 1s 14ms/step - loss: 0.0325 - val_loss: 0.0322\n","Fitting 7/12 model\n","Epoch 1/20\n","46/46 [==============================] - 2s 21ms/step - loss: 0.0707 - val_loss: 0.0403\n","Epoch 2/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0575 - val_loss: 0.0357\n","Epoch 3/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0531 - val_loss: 0.0333\n","Epoch 4/20\n","46/46 [==============================] - 0s 9ms/step - loss: 0.0466 - val_loss: 0.0327\n","Epoch 5/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0414 - val_loss: 0.0332\n","Epoch 6/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0330 - val_loss: 0.0342\n","Epoch 7/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0290 - val_loss: 0.0357\n","Epoch 8/20\n","46/46 [==============================] - 0s 9ms/step - loss: 0.0392 - val_loss: 0.0358\n","Epoch 9/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0395 - val_loss: 0.0358\n","Epoch 10/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0333 - val_loss: 0.0363\n","Epoch 11/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0367 - val_loss: 0.0362\n","Epoch 12/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0357 - val_loss: 0.0360\n","Epoch 13/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0368 - val_loss: 0.0364\n","Epoch 14/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0390 - val_loss: 0.0361\n","Epoch 15/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0364 - val_loss: 0.0364\n","Epoch 16/20\n","46/46 [==============================] - 0s 11ms/step - loss: 0.0367 - val_loss: 0.0368\n","Epoch 17/20\n","46/46 [==============================] - 0s 9ms/step - loss: 0.0369 - val_loss: 0.0365\n","Epoch 18/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0330 - val_loss: 0.0359\n","Epoch 19/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0381 - val_loss: 0.0358\n","Epoch 20/20\n","46/46 [==============================] - 0s 10ms/step - loss: 0.0333 - val_loss: 0.0363\n","Fitting 8/12 model\n","Epoch 1/20\n","46/46 [==============================] - 7s 50ms/step - loss: 0.0848 - val_loss: 0.0461\n","Epoch 2/20\n","46/46 [==============================] - 1s 23ms/step - loss: 0.0672 - val_loss: 0.0400\n","Epoch 3/20\n","46/46 [==============================] - 1s 23ms/step - loss: 0.0503 - val_loss: 0.0360\n","Epoch 4/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0388 - val_loss: 0.0356\n","Epoch 5/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0376 - val_loss: 0.0361\n","Epoch 6/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0332 - val_loss: 0.0370\n","Epoch 7/20\n","46/46 [==============================] - 1s 25ms/step - loss: 0.0312 - val_loss: 0.0370\n","Epoch 8/20\n","46/46 [==============================] - 1s 25ms/step - loss: 0.0380 - val_loss: 0.0367\n","Epoch 9/20\n","46/46 [==============================] - 1s 25ms/step - loss: 0.0323 - val_loss: 0.0373\n","Epoch 10/20\n","46/46 [==============================] - 1s 26ms/step - loss: 0.0373 - val_loss: 0.0377\n","Epoch 11/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0344 - val_loss: 0.0372\n","Epoch 12/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0316 - val_loss: 0.0379\n","Epoch 13/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0363 - val_loss: 0.0379\n","Epoch 14/20\n","46/46 [==============================] - 1s 25ms/step - loss: 0.0372 - val_loss: 0.0373\n","Epoch 15/20\n","46/46 [==============================] - 1s 23ms/step - loss: 0.0395 - val_loss: 0.0371\n","Epoch 16/20\n","46/46 [==============================] - 1s 23ms/step - loss: 0.0333 - val_loss: 0.0373\n","Epoch 17/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0425 - val_loss: 0.0367\n","Epoch 18/20\n","46/46 [==============================] - 1s 23ms/step - loss: 0.0410 - val_loss: 0.0367\n","Epoch 19/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0405 - val_loss: 0.0367\n","Epoch 20/20\n","46/46 [==============================] - 1s 24ms/step - loss: 0.0357 - val_loss: 0.0379\n","Fitting 9/12 model\n","Epoch 1/20\n","46/46 [==============================] - 4s 35ms/step - loss: 0.0787 - val_loss: 0.0458\n","Epoch 2/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0595 - val_loss: 0.0387\n","Epoch 3/20\n","46/46 [==============================] - 1s 18ms/step - loss: 0.0495 - val_loss: 0.0351\n","Epoch 4/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0445 - val_loss: 0.0347\n","Epoch 5/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0398 - val_loss: 0.0353\n","Epoch 6/20\n","46/46 [==============================] - 1s 18ms/step - loss: 0.0355 - val_loss: 0.0362\n","Epoch 7/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0366 - val_loss: 0.0370\n","Epoch 8/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0367 - val_loss: 0.0369\n","Epoch 9/20\n","46/46 [==============================] - 1s 16ms/step - loss: 0.0317 - val_loss: 0.0382\n","Epoch 10/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0333 - val_loss: 0.0377\n","Epoch 11/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0329 - val_loss: 0.0376\n","Epoch 12/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0331 - val_loss: 0.0379\n","Epoch 13/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0388 - val_loss: 0.0381\n","Epoch 14/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0377 - val_loss: 0.0378\n","Epoch 15/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0379 - val_loss: 0.0375\n","Epoch 16/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0294 - val_loss: 0.0386\n","Epoch 17/20\n","46/46 [==============================] - 1s 18ms/step - loss: 0.0289 - val_loss: 0.0389\n","Epoch 18/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0329 - val_loss: 0.0387\n","Epoch 19/20\n","46/46 [==============================] - 1s 18ms/step - loss: 0.0332 - val_loss: 0.0389\n","Epoch 20/20\n","46/46 [==============================] - 1s 17ms/step - loss: 0.0356 - val_loss: 0.0381\n","Fitting 10/12 model\n","Epoch 1/20\n","26/26 [==============================] - 2s 38ms/step - loss: 0.0637 - val_loss: 0.0081\n","Epoch 2/20\n","26/26 [==============================] - 0s 18ms/step - loss: 0.0583 - val_loss: 0.0062\n","Epoch 3/20\n","26/26 [==============================] - 0s 18ms/step - loss: 0.0529 - val_loss: 0.0049\n","Epoch 4/20\n","26/26 [==============================] - 1s 19ms/step - loss: 0.0475 - val_loss: 0.0041\n","Epoch 5/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0463 - val_loss: 0.0039\n","Epoch 6/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0422 - val_loss: 0.0043\n","Epoch 7/20\n","26/26 [==============================] - 1s 20ms/step - loss: 0.0401 - val_loss: 0.0053\n","Epoch 8/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0380 - val_loss: 0.0069\n","Epoch 9/20\n","26/26 [==============================] - 0s 18ms/step - loss: 0.0365 - val_loss: 0.0093\n","Epoch 10/20\n","26/26 [==============================] - 1s 20ms/step - loss: 0.0340 - val_loss: 0.0128\n","Epoch 11/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0356 - val_loss: 0.0166\n","Epoch 12/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0373 - val_loss: 0.0205\n","Epoch 13/20\n","26/26 [==============================] - 1s 20ms/step - loss: 0.0348 - val_loss: 0.0231\n","Epoch 14/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0389 - val_loss: 0.0247\n","Epoch 15/20\n","26/26 [==============================] - 0s 18ms/step - loss: 0.0404 - val_loss: 0.0263\n","Epoch 16/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0400 - val_loss: 0.0273\n","Epoch 17/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0408 - val_loss: 0.0280\n","Epoch 18/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0387 - val_loss: 0.0288\n","Epoch 19/20\n","26/26 [==============================] - 1s 19ms/step - loss: 0.0400 - val_loss: 0.0298\n","Epoch 20/20\n","26/26 [==============================] - 0s 19ms/step - loss: 0.0393 - val_loss: 0.0308\n","Fitting 11/12 model\n","Epoch 1/20\n","26/26 [==============================] - 6s 95ms/step - loss: 0.0493 - val_loss: 0.0038\n","Epoch 2/20\n","26/26 [==============================] - 1s 46ms/step - loss: 0.0430 - val_loss: 0.0039\n","Epoch 3/20\n","26/26 [==============================] - 1s 46ms/step - loss: 0.0380 - val_loss: 0.0054\n","Epoch 4/20\n","26/26 [==============================] - 1s 46ms/step - loss: 0.0380 - val_loss: 0.0080\n","Epoch 5/20\n","26/26 [==============================] - 1s 44ms/step - loss: 0.0364 - val_loss: 0.0115\n","Epoch 6/20\n","26/26 [==============================] - 1s 45ms/step - loss: 0.0341 - val_loss: 0.0164\n","Epoch 7/20\n","26/26 [==============================] - 1s 47ms/step - loss: 0.0375 - val_loss: 0.0198\n","Epoch 8/20\n","26/26 [==============================] - 1s 45ms/step - loss: 0.0368 - val_loss: 0.0225\n","Epoch 9/20\n","26/26 [==============================] - 1s 47ms/step - loss: 0.0409 - val_loss: 0.0233\n","Epoch 10/20\n","26/26 [==============================] - 1s 45ms/step - loss: 0.0400 - val_loss: 0.0225\n","Epoch 11/20\n","26/26 [==============================] - 1s 45ms/step - loss: 0.0349 - val_loss: 0.0257\n","Epoch 12/20\n","26/26 [==============================] - 1s 44ms/step - loss: 0.0404 - val_loss: 0.0263\n","Epoch 13/20\n","26/26 [==============================] - 1s 43ms/step - loss: 0.0402 - val_loss: 0.0268\n","Epoch 14/20\n","26/26 [==============================] - 1s 45ms/step - loss: 0.0490 - val_loss: 0.0231\n","Epoch 15/20\n","26/26 [==============================] - 1s 43ms/step - loss: 0.0457 - val_loss: 0.0218\n","Epoch 16/20\n","26/26 [==============================] - 1s 44ms/step - loss: 0.0387 - val_loss: 0.0233\n","Epoch 17/20\n","26/26 [==============================] - 1s 44ms/step - loss: 0.0446 - val_loss: 0.0216\n","Epoch 18/20\n","26/26 [==============================] - 1s 44ms/step - loss: 0.0406 - val_loss: 0.0206\n","Epoch 19/20\n","26/26 [==============================] - 1s 45ms/step - loss: 0.0416 - val_loss: 0.0198\n","Epoch 20/20\n","26/26 [==============================] - 1s 44ms/step - loss: 0.0403 - val_loss: 0.0203\n","Fitting 12/12 model\n","Epoch 1/20\n","26/26 [==============================] - 4s 63ms/step - loss: 0.0599 - val_loss: 0.0057\n","Epoch 2/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0477 - val_loss: 0.0040\n","Epoch 3/20\n","26/26 [==============================] - 1s 33ms/step - loss: 0.0432 - val_loss: 0.0041\n","Epoch 4/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0381 - val_loss: 0.0055\n","Epoch 5/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0377 - val_loss: 0.0081\n","Epoch 6/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0356 - val_loss: 0.0116\n","Epoch 7/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0365 - val_loss: 0.0150\n","Epoch 8/20\n","26/26 [==============================] - 1s 31ms/step - loss: 0.0354 - val_loss: 0.0194\n","Epoch 9/20\n","26/26 [==============================] - 1s 31ms/step - loss: 0.0407 - val_loss: 0.0203\n","Epoch 10/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0398 - val_loss: 0.0220\n","Epoch 11/20\n","26/26 [==============================] - 1s 30ms/step - loss: 0.0349 - val_loss: 0.0247\n","Epoch 12/20\n","26/26 [==============================] - 1s 33ms/step - loss: 0.0423 - val_loss: 0.0252\n","Epoch 13/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0418 - val_loss: 0.0264\n","Epoch 14/20\n","26/26 [==============================] - 1s 34ms/step - loss: 0.0385 - val_loss: 0.0274\n","Epoch 15/20\n","26/26 [==============================] - 1s 31ms/step - loss: 0.0439 - val_loss: 0.0264\n","Epoch 16/20\n","26/26 [==============================] - 1s 34ms/step - loss: 0.0403 - val_loss: 0.0265\n","Epoch 17/20\n","26/26 [==============================] - 1s 31ms/step - loss: 0.0382 - val_loss: 0.0275\n","Epoch 18/20\n","26/26 [==============================] - 1s 31ms/step - loss: 0.0402 - val_loss: 0.0272\n","Epoch 19/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0396 - val_loss: 0.0267\n","Epoch 20/20\n","26/26 [==============================] - 1s 32ms/step - loss: 0.0372 - val_loss: 0.0277\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFJmKllpR83a","executionInfo":{"status":"ok","timestamp":1620248008236,"user_tz":300,"elapsed":12384,"user":{"displayName":"Chaitra Srirama","photoUrl":"","userId":"06885739003473900788"}},"outputId":"3ceb9005-773e-4889-8065-6fa6445c2886"},"source":["###Choose the bets model and retrain with the best parameters.\n","\n","params_grid = {'input_size': [20],\n","              'hidden_units':[100],\n","              'dropout': [True],\n","              'learning_rate':[4e-5],\n","              'n_ahead':[12],\n","              'val_split': [0.2],\n","              'epochs':[20],\n","              'verbose':[False],\n","              'plot':[False]}\n","\n","\n","#Re-Training the model using the best parameters\n","model_, logs, best_predictions, y_test = GridSearch(Risk_Score,params_grid)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 1/1 model\n","Epoch 1/20\n","50/50 [==============================] - 2s 19ms/step - loss: 0.0419 - val_loss: 0.0343\n","Epoch 2/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0378 - val_loss: 0.0306\n","Epoch 3/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0312 - val_loss: 0.0290\n","Epoch 4/20\n","50/50 [==============================] - 0s 10ms/step - loss: 0.0306 - val_loss: 0.0289\n","Epoch 5/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0254 - val_loss: 0.0294\n","Epoch 6/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0368 - val_loss: 0.0294\n","Epoch 7/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0328 - val_loss: 0.0298\n","Epoch 8/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0330 - val_loss: 0.0299\n","Epoch 9/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0407 - val_loss: 0.0299\n","Epoch 10/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0280 - val_loss: 0.0304\n","Epoch 11/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0306 - val_loss: 0.0310\n","Epoch 12/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0327 - val_loss: 0.0310\n","Epoch 13/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0283 - val_loss: 0.0319\n","Epoch 14/20\n","50/50 [==============================] - 0s 8ms/step - loss: 0.0313 - val_loss: 0.0318\n","Epoch 15/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0337 - val_loss: 0.0315\n","Epoch 16/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0373 - val_loss: 0.0311\n","Epoch 17/20\n","50/50 [==============================] - 0s 8ms/step - loss: 0.0324 - val_loss: 0.0312\n","Epoch 18/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0341 - val_loss: 0.0311\n","Epoch 19/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0318 - val_loss: 0.0312\n","Epoch 20/20\n","50/50 [==============================] - 0s 9ms/step - loss: 0.0346 - val_loss: 0.0316\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4EV1e_zDSPJP"},"source":["### Choose the best parameters for the model\n","params = {'input_size': 20,\n","          'hidden_units':100,\n","          'dropout': True,\n","          'learning_rate':4e-5,\n","          'n_ahead':12,\n","          'val_split': 0.2,\n","          'epochs':20,\n","          'verbose':False,\n","          'plot':False}\n","\n","#### Remove repositories with less than 20 weeks of data and other erroneous repositories \n","repo_list=list(Dataset['repoID'].unique())\n","\n","rows=[]\n","for i in range(0, len(repo_list)):\n","  repo_data=Dataset.loc[Dataset['repoID']==repo_list[i]]\n","  if repo_data.shape[0]>20:\n","    rows.append((repo_list[i]))\n","\n","remove_list=[156630873, 168234055, 163867485, 173968989, 169825932, 165114031, 121291048, 178027217, 194736160, 181737603, 194727787, 149379192, 194736246, 159751068, 193533462, 53032138, 169601732, 173919375, 170547737, 178105940, 196555066, 141831152, 172554606, 157298123, 159539290, 185833148,164036674, 164198539, 159058750, 183677088, 145325821]\n","res = [i for i in rows if i not in remove_list]\n","rows=res   #final number of repos 221."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUdEM4pvSz3d","colab":{"base_uri":"https://localhost:8080/","height":989},"executionInfo":{"status":"ok","timestamp":1620248042535,"user_tz":300,"elapsed":41186,"user":{"displayName":"Chaitra Srirama","photoUrl":"","userId":"06885739003473900788"}},"outputId":"3f3c75b1-cf7b-4d54-fef5-03e0c263f546"},"source":["#### Prediction Block\n","\n","#### Input a repository ID\n","repo_data=Dataset.loc[Dataset['repoID']==11362691]\n","single_repo_series=repo_data.Normalised_Activity_Score\n","Risk_Score=single_repo_series\n","\n","new_model_, mse, history, predictions, y_test = Test_FitEvaluate(Risk_Score, params, model_)\n","\n","#### plotting predictions for next 12 weeks\n","\n","Final_Score=predictions.reshape(12)\n","Final_Score[Final_Score<0] = 0\n","\n","#### data to be plotted \n","x = np.arange(start=1, stop=13, step=1)\n","\n","#print(i)\n","#print(rows[i])\n","fig = plt.figure()\n","fig.patch.set_facecolor('white') \n","plt.title(\"Activity Score for the next 12 weeks\")  \n","plt.xlabel(\"Date\")  \n","plt.ylabel(\"Activity-Score\")\n","plt.plot(x, Final_Score, color =\"green\")  \n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","196/196 [==============================] - 5s 10ms/step - loss: 0.0351 - val_loss: 0.0243\n","Epoch 2/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0330 - val_loss: 0.0240\n","Epoch 3/20\n","196/196 [==============================] - 2s 8ms/step - loss: 0.0340 - val_loss: 0.0237\n","Epoch 4/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0237\n","Epoch 5/20\n","196/196 [==============================] - 2s 8ms/step - loss: 0.0330 - val_loss: 0.0238\n","Epoch 6/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0238\n","Epoch 7/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0340 - val_loss: 0.0238\n","Epoch 8/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0334 - val_loss: 0.0238\n","Epoch 9/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0332 - val_loss: 0.0238\n","Epoch 10/20\n","196/196 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0237\n","Epoch 11/20\n","196/196 [==============================] - 1s 8ms/step - loss: 0.0335 - val_loss: 0.0237\n","Epoch 12/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0236\n","Epoch 13/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0335 - val_loss: 0.0237\n","Epoch 14/20\n","196/196 [==============================] - 2s 8ms/step - loss: 0.0341 - val_loss: 0.0236\n","Epoch 15/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0337 - val_loss: 0.0236\n","Epoch 16/20\n","196/196 [==============================] - 2s 8ms/step - loss: 0.0337 - val_loss: 0.0237\n","Epoch 17/20\n","196/196 [==============================] - 1s 7ms/step - loss: 0.0342 - val_loss: 0.0237\n","Epoch 18/20\n","196/196 [==============================] - 1s 8ms/step - loss: 0.0331 - val_loss: 0.0237\n","Epoch 19/20\n","196/196 [==============================] - 1s 8ms/step - loss: 0.0334 - val_loss: 0.0236\n","Epoch 20/20\n","196/196 [==============================] - 1s 8ms/step - loss: 0.0336 - val_loss: 0.0236\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"Xw01hcM8NNf5"},"source":[""],"execution_count":null,"outputs":[]}]}